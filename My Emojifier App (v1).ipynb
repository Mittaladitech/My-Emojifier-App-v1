{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Emojify \n",
    "\n",
    "We are going to use word vector representations to build an Emojifier. \n",
    "\n",
    "If we want to make our text messages more expressive? Our emojifier app will help us do that. \n",
    "So rather than writing:\n",
    ">\"Congratulations on the promotion! Let's get coffee and talk. Love you!\"   \n",
    "\n",
    "The emojifier can automatically turn this into:\n",
    ">\"Congratulations on the promotion! üëç Let's get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\"\n",
    "\n",
    "* We will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1\n",
    "\n",
    "Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a simple baseline classifier. \n",
    "\n",
    "We have a tiny dataset (X, Y) where:\n",
    "- X contains 127 sentences (strings).\n",
    "- Y contains an integer label between 0 and 4 corresponding to an emoji for each sentence.\n",
    "\n",
    "<img src=\"data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n",
    "Let's load the dataset using the code below. We will split the dataset between training (127 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will print sentences from X_train and corresponding labels from Y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again üòû\n",
      "I am proud of your achievements üòÑ\n",
      "It is the worst day in my life üòû\n",
      "Miss you so much ‚ù§Ô∏è\n",
      "food is life üç¥\n",
      "I love you mum ‚ù§Ô∏è\n",
      "Stop saying bullshit üòû\n",
      "congratulations on your acceptance üòÑ\n",
      "The assignment is too long  üòû\n",
      "I want to go play ‚öæ\n",
      "she did not answer my text  üòû\n",
      "Your stupidity has no limit üòû\n",
      "how many points did he score ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for idx in range(13):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will implement a baseline model  \n",
    "\n",
    "<center>\n",
    "<img src=\"basemodel.png\" style=\"width:900px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Baseline model (Emojifier-V1).</center></caption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'I missed you' has label index 4, which is emoji üç¥\n",
      "Label index 4 in one-hot encoding format is [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what `convert_to_one_hot()` did. \n",
    "\n",
    "idx = 26\n",
    "print(f\"Sentence '{X_train[50]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell will load the `word_to_vec_map`, which contains all the vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "# Call the function\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `word_to_index`: dictionary mapping from words to their indices in the vocabulary \n",
    "    - (400,001 words, with the valid indices ranging from 0 to 400,000)\n",
    "- `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "#  Run the following cell to check if it works.\n",
    "word = \"cucumber\"\n",
    "idx = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Split sentence into list of lower case words (‚âà 1 line)\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    avg = np.zeros((len(word_to_vec_map, )))\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        total += word_to_vec_map[w]\n",
    "    avg = total/len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): # Loop over the number of iterations\n",
    "        for i in range(m):          # Loop over the training examples\n",
    "\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -1 * np.sum(Y_oh[i] * np.log(a))\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "Epoch: 100 --- cost = 0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 200 --- cost = 0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "Epoch: 300 --- cost = 0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n",
      "[[3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# Train our model and learn the softmax parameters (W,b).\n",
    "\n",
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9772727272727273\n",
      "Test set:\n",
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Examining test set performance \n",
    "\n",
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you ‚ù§Ô∏è\n",
      "i love you ‚ù§Ô∏è\n",
      "funny lol üòÑ\n",
      "lets play with a ball ‚öæ\n",
      "food is ready üç¥\n",
      "not feeling happy üòÑ\n"
     ]
    }
   ],
   "source": [
    "# The model matches emojis to relevant words\n",
    "\n",
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "* A confusion matrix shows how often an example whose label is one class (\"actual\" class) is mislabeled by the algorithm with a different class (\"predicted\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56,)\n",
      "           ‚ù§Ô∏è    ‚öæ    üòÑ    üòû   üç¥\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            6    0    0    1    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            2    0   16    0    0   18\n",
      "3            1    1    2   12    0   16\n",
      "4            0    0    1    0    6    7\n",
      "All          9    9   19   13    6   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1db5848d128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
